As described in the Readme documentation, the scheduler will perform weighted random selection based on the score values of different inference engine service nodes. This document verifies the effectiveness of the scheduler through end-to-end testing methods.

## 1. End-to-End Testing Evaluation Method

### F5 Mock API

`./tests/f5-mock-api/f5-mock-api-updated.py` is used to simulate F5 iControl Rest. Start this program:

```
python3 f5-mock-api-update.py
```

This mock API can simulate login authentication, token acquisition, and obtaining member information for two pools (`example_pool1`, `example_pool2`). The `DYNAMIC_MODE = 0` in the program indicates that it does not simulate the addition or removal of pool members, always maintaining the 3 members set in the program. If `DYNAMIC_MODE = 1` is set, the program will randomly simulate the addition and removal of pool members (0-3 members).

### LLM Mock API

`./tests/llm-mock-api/main.py` is the main program that simulates vLLM's metrics interface, and `./tests/llm-mock-api/main_sglang.py` simulates SGlang's metrics interface. The main program will start 3 processes each, with each process simulating an LLM Inference server. Start the programs:

```
python3 main.py
python3 main_sglang.py
```

The process program (such as app_vllm.py) changes metrics values automatically every 10 minutes by default. You can also actively trigger metrics updates through the `/trigger_update` API interface. Modify `DYNAMIC_UPDATE_ENABLED = True` to `False` in the program to disable dynamic metrics updates. The dynamic update switch can also be controlled externally through API endpoints `POST /toggle_update/1 or /toggle_update/0`.

> You need start the above 2 API first before the subsequent steps.

### Client Testing Program

`./tests/Client_requests_test_and_caculator_full_report.py` is used to simulate requests to the scheduler. When the program starts, it will ask the user how many rounds of testing to simulate. Assuming the user inputs 30, this means 30 rounds of testing will be conducted, with each round following this logic:

- Send 1000 API requests to the scheduler. Each request will return member information from the scheduler (based on the weighted random algorithm). During these 1000 requests, the LLM metrics remain unchanged.
- After completing 1000 requests, the program counts the actual percentage probability of each member appearing.
- The program obtains the score values of each LLM Inference server in this round through the scheduler's `/pools/status` API endpoint to calculate the theoretical percentage probability of each server.
- The program sends one request to the LLM mock API's `/trigger_update` endpoint to trigger a metrics value update for the next round of testing.

After the program runs, it will automatically generate visualization chart files.

> Note: If running on a remote terminal, please comment out this statement in the python script: 
>
> plt.show() 

Each round of testing terminal output is similar to the following:

```
=== Round 1 test begins ===
Test results statistics:
---------------------
Total requests: 1000
Successful requests: 1000
Failed requests: 0
#First column is member, second column is selection probability, third column is percent field from /pools/status interface
----Member selection probability and score distribution probability----
127.0.0.1:8001,33.60%,32.04%
127.0.0.1:8002,62.40%,63.11%
127.0.0.1:8003,4.00%,4.85%
--------------------------------------------
Pausing 1 second, preparing to trigger metrics update...
All metrics update requests executed successfully.

```

### Program Effectiveness Evaluation Results (End-to-End Testing)

Through simulating 30 rounds (1000 requests per round), the actual probability and theoretical probability of each Pool member being selected are as follows.

Legend:

- Blue represents actual selection probability
- Orange represents theoretical probability

It can be observed that the actual selection results of the program have small deviations from the theoretical values, meeting expectations.

![127.0.0.1_8001_trend](Program-effectiveness-Evaluation.assets/127.0.0.1_8001_trend.png)

![127.0.0.1_8002_trend](Program-effectiveness-Evaluation.assets/127.0.0.1_8002_trend.png)

![127.0.0.1_8003_trend](Program-effectiveness-Evaluation.assets/127.0.0.1_8003_trend.png)

## 2. Evaluation Method Through Scheduler's Own API Interface

The scheduler itself provides an `analyze` API endpoint that can automatically simulate within the program and provide actual probability, theoretical probability, and deviation analysis (mean deviation, maximum deviation, minimum deviation, standard deviation) for each inference node selection. Based on the evaluation criteria in the appendix, it provides final quality assessment. API interface access:

```
curl -X POST http://localhost:8080/pools/example_pool1/Common/analyze \                                                                            on git:main|â€¦6
   -H "Content-Type: application/json" \
   -d '{
     "pool_name": "example_pool1",
     "partition": "Common",
     "request": "ScheduleRequest",
     "members": ["127.0.0.1:8001", "127.0.0.1:8002", "127.0.0.1:8003"],
     "iterations": 100
   }'
```

The output result is similar to the following:

```
  ...
  "overall_statistics": {
    "total_iterations": 1000,
    "successful_selections": 1000,
    "success_rate": 100.0,
    "mean_absolute_deviation": 0.7969,
    "max_absolute_deviation": 1.1954,
    "min_absolute_deviation": 0.121,
    "std_deviation": 0.5885
  },
  "quality_assessment": {
    "quality_grade": "Excellent",
    "quality_score": 99.2,
    "is_acceptable": true,
    "recommendations": [],
    "summary": "Mean deviation 0.7969%, max deviation 1.1954%, quality grade: Excellent"
  },
  ...
```

## 3. If you are Chinese usersï¼Œè¿˜å¯ä»¥ä½¿ç”¨å¦‚ä¸‹è„šæœ¬è¿›è¡Œæ›´ç›´è§‚çš„è¯„æµ‹

æ‰§è¡Œ`tests/test_probability_analysis.py`ç¨‹åºï¼Œå‡ºç°å¦‚ä¸‹æç¤ºï¼Œé€‰æ‹©1å¯ä»¥ä¸éœ€è¦llm-mock-apiçš„è¿è¡Œï¼Œå¦‚é€‰æ‹©2åˆ™éœ€è¦llm-mock-apiè¿è¡Œã€‚

```
ğŸ¯ åŠ æƒéšæœºç®—æ³•æ¦‚ç‡åˆ†ææµ‹è¯•å·¥å…·
==================================================
ä»¥ä¸‹APIæ¬¡æ•°æ˜¯æŒ‡è°ƒç”¨http://localhost:8080/pools/example_pool1/Common/analyzeæ¥å£
è¿­ä»£æ¬¡æ•°æ˜¯æŒ‡è¯¥APIå†…éƒ¨æ¨¡æ‹Ÿé€‰æ‹©æ‰€æ‰§è¡Œçš„æ¬¡æ•°
æŠ¥å‘Šä¸­çš„å…³äºè¿­ä»£æ¬¡æ•°çš„è€—æ—¶èƒ½ååº”ç¨‹åºå†…éƒ¨é€‰æ‹©ç®—æ³•çš„è€—æ—¶
APIçš„è€—æ—¶åŠé€Ÿç‡ä¸èƒ½ä»£è¡¨çœŸå®çš„/scheduler/select APIæ¥å£æ€§èƒ½
==================================================
è¯·é€‰æ‹©æµ‹è¯•æ–¹å¼:
1. æœ¬åœ°æµ‹è¯•ï¼ˆç›´æ¥è°ƒç”¨ç®—æ³•ï¼‰
2. APIæµ‹è¯•ï¼ˆé€šè¿‡HTTPæ¥å£ï¼‰,éœ€è¦ç¯å¢ƒæä¾›æ¨¡æ‹Ÿexample_pool1çš„3ä¸ªæˆå‘˜
3. ç®—æ³•å¯¹æ¯”æµ‹è¯•
4. å…¨éƒ¨æµ‹è¯•
5. è‡ªå®šä¹‰iterationsçš„APIæµ‹è¯•
6. å¤šæ¬¡APIè¯·æ±‚æ€§èƒ½æµ‹è¯•

è¯·è¾“å…¥é€‰æ‹© (1-6): 1
```

é€‰é¡¹1çš„æµ‹è¯•ï¼Œç»“æœç±»ä¼¼å¦‚ä¸‹ï¼š

```

ğŸ“Š æ¦‚ç‡åˆ†æç»“æœ:
================================================================================
Pool: example_pool1:Common
æˆå‘˜æ•°: 3, æ€»Score: 1.5

ğŸ“ˆ æ¦‚ç‡å¯¹æ¯”:
------------------------------------------------------------
æˆå‘˜                   ç†è®ºæ¦‚ç‡         å®é™…æ¦‚ç‡         ç»å¯¹åå·®
------------------------------------------------------------
127.0.0.1:8001       58.67      % 59.00      % 0.33       %
127.0.0.1:8002       33.33      % 32.30      % 1.03       %
127.0.0.1:8003       8.00       % 8.70       % 0.70       %

ğŸ“Š æ•´ä½“ç»Ÿè®¡:
  æ€»è¿­ä»£æ¬¡æ•°: 1000
  æˆåŠŸé€‰æ‹©æ¬¡æ•°: 1000
  æˆåŠŸç‡: 100.00%
  å¹³å‡ç»å¯¹åå·®: 0.69%
  æœ€å¤§ç»å¯¹åå·®: 1.03%
  æœ€å°ç»å¯¹åå·®: 0.33%
  åå·®æ ‡å‡†å·®: 0.35%

â­ è´¨é‡è¯„ä¼°:
  è´¨é‡ç­‰çº§: Excellent
  è´¨é‡è¯„åˆ†: 99.31
  æ˜¯å¦å¯æ¥å—: æ˜¯
  æ€»ç»“: Mean deviation 0.6889%, max deviation 1.0333%, quality grade: Excellent
```

### Appendix

#### Quality Grade Definitions

| Grade              | Mean Deviation | Max Deviation | Success Rate | Evaluation                                              |
| ------------------ | -------------- | ------------- | ------------ | ------------------------------------------------------- |
| **Excellent**      | < 1.0%         | < 2.0%        | > 99%        | Algorithm performs excellently, deviation within normal statistical range |
| **Good**           | < 2.0%         | < 5.0%        | > 95%        | Algorithm performs well, deviation is acceptable        |
| **Average**        | < 5.0%         | < 10.0%       | > 90%        | Algorithm is basically usable, with room for optimization |
| **Needs Optimization** | â‰¥ 5.0%    | â‰¥ 10.0%       | â‰¤ 90%        | Algorithm needs optimization or has issues              |

#### Recommended Thresholds

- **1000 tests**: Mean deviation < 2%, max deviation < 5% is considered good
- **10000 tests**: Mean deviation < 1%, max deviation < 3% is considered excellent